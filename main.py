
"""
CLI entry point for the movie recommendation service.
"""

from langchain.schema import Document
from langchain.vectorstores import FAISS

from database import *
from utils import *
from recommender import *

from vector_db import build_vectorstore, build_qa_chain
from data_loader import load_dataframe
from config import model_name, embedding_model_name
from langchain.embeddings import OpenAIEmbeddings
from langchain.chat_models import ChatOpenAI
from langchain.chains import ConversationalRetrievalChain

from typing import List

# Global models
embedding_model = OpenAIEmbeddings(model=embedding_model_name)
llm = ChatOpenAI(model_name=model_name, temperature=0.3)

# Load data at startup
df = load_dataframe()
# Todo: ì‚­ì œ
df = df[:100]
vectorstore_global = build_vectorstore(df["document"].apply(lambda x: Document(page_content=x, metadata={})).tolist())
qa_chain = build_qa_chain(vectorstore_global)

def main_chat_loop():
    user_name = input("ğŸ‘‹ ë‹¹ì‹ ì˜ ì´ë¦„ì„ ì•Œë ¤ì£¼ì„¸ìš”: ").strip()
    user_id = get_or_create_user_id(user_name)
    print(f"{user_name}ë‹˜ ì•ˆë…•í•˜ì„¸ìš”? ì˜í™” ì¶”ì²œì„ ì‹œì‘í•©ë‹ˆë‹¤ ğŸ˜Š")

    selected_title = None
    interaction_id = None
    last_recommend_df = None
    last_recommend_query = None
    last_recommend_type = None
    first_turn = True
    last_selected_title = None

    while True:
        query = input("\nâ“ ì‚¬ìš©ì: ").strip()

        if query.lower() in ["exit", "quit", "ì¢…ë£Œ"]:
            print("ğŸ‘‹ ëŒ€í™”ë¥¼ ì¢…ë£Œí•©ë‹ˆë‹¤.")
            break

        interaction_id = create_interaction(user_id, query)

        # âœ… ì™„ë£Œ ì²˜ë¦¬
        if "ì™„ë£Œ" in query:
            if last_recommend_df is None or last_recommend_df.empty:
                print("âš ï¸ ì´ì „ì— ì¶”ì²œëœ ì˜í™”ê°€ ì—†ìŠµë‹ˆë‹¤. ë¨¼ì € ì¶”ì²œì„ ë°›ì•„ì£¼ì„¸ìš”.")
                continue
            if last_recommend_df is not None and not last_recommend_df.empty:
                selected_title = handle_completion(query, last_recommend_df, interaction_id, user_id)
                if selected_title:
                    break
                else:
                    continue

        # âœ… í›„ì† ì§ˆë¬¸ ì²˜ë¦¬
        if not first_turn and last_recommend_df is not None and not last_recommend_df.empty:
            if is_follow_up_question(query, last_recommend_df["title"].tolist()):
                print("ğŸ“Œ í›„ì† ì§ˆë¬¸ìœ¼ë¡œ íŒë‹¨ë¨ â†’ ì´ì „ ì¶”ì²œ ì½˜í…ì¸ ì—ì„œ ê²€ìƒ‰ ì¤‘...")
                local_docs = [
                    Document(page_content=truncate_document(row["document"]), metadata={"title": row["title"]})
                    for _, row in last_recommend_df.iterrows()
                ]
                local_store = FAISS.from_documents(local_docs, embedding_model)
                local_chain = ConversationalRetrievalChain.from_llm(
                    llm=llm,
                    retriever=local_store.as_retriever(),
                    memory=None
                )
                result = local_chain.invoke({"question": query, "chat_history": []})
                print("ğŸ¤– GPT:\n", result["answer"])
                continue

            if is_similar_recommendation(query):
                print("is_similar_recommendation")
                df_recommend = handle_similar_recommendation(query, df, user_id, selected_title, extract_user_meta, keyword_columns)
                if df_recommend.empty:
                    print("ğŸ¤– GPT:\nì£„ì†¡í•´ìš”, ìœ ì‚¬í•œ ì½˜í…ì¸ ë¥¼ ì°¾ì§€ ëª»í–ˆì–´ìš”.")
                    continue
                response_text = generate_recommendation_response(query, df_recommend, user_name)
                print("ğŸ¤– GPT:\n", response_text)
                log_recommendations(interaction_id, df_recommend["title"].tolist())
                last_recommend_df = df_recommend.copy()
                last_recommend_query = query
                continue

            is_retry, retry_mode = is_retry_request(query)
            if is_retry and last_recommend_query:
                # â”€â”€ 1) ì¬ì¶”ì²œ ë¶„ê¸° â”€â”€
                print(f"ğŸ” ì¬ì¶”ì²œ ìš”ì²­ ({retry_mode}) â†’ ì´ì „ ì¿¼ë¦¬ë¡œ í‚¤ì›Œë“œ ì¶”ì¶œ: {last_recommend_query}")
                # (1) merged_queryëŠ” ì´ì „ ì¿¼ë¦¬ ì¬ì‚¬ìš©
                merged_query = last_recommend_query
                # (2) user_metaëŠ” ì§ì „ì— ì €ì¥í•´ ë‘” ë©”íƒ€ ì¬í™œìš©
                user_meta = extract_user_meta(merged_query)
                last_user_meta    = user_meta
                # (3) ì¬ì¶”ì²œ í•¸ë“¤ëŸ¬ í˜¸ì¶œ (selected_titleì´ ìˆìœ¼ë©´ ë„˜ê²¨ì£¼ê³ , ì—†ìœ¼ë©´ None)
                last_selected_title=df_recommend["title"].tolist()
                df_recommend = handle_recommendation(df,user_id,user_meta,  selected_title=last_selected_title   )
            else:
                # â”€â”€ 2) ì²« ì¶”ì²œ ë¶„ê¸° â”€â”€
                merged_query = query
                # (1) queryì—ì„œ í•œ ë²ˆë§Œ ë©”íƒ€ ì¶”ì¶œ
                user_meta = extract_user_meta(merged_query)
                # (2) ì„¸ì…˜ì— ì €ì¥
                last_user_meta = user_meta
                last_recommend_query = merged_query
                # (3) ì²« ì¶”ì²œ í•¸ë“¤ëŸ¬ í˜¸ì¶œ
                df_recommend = handle_recommendation(
                    df,
                    user_id=user_id,
                    user_meta=user_meta
                )

            # â”€â”€ ê³µí†µ: ì¶”ì²œ ê²°ê³¼ ì¶œë ¥ ë° ìƒíƒœ ì—…ë°ì´íŠ¸ â”€â”€
            if df_recommend is None or df_recommend.empty:
                print("ğŸ¤– GPT:\nì£„ì†¡í•´ìš”, ì¶”ì²œí•  ì½˜í…ì¸ ë¥¼ ì°¾ì§€ ëª»í–ˆì–´ìš”.")
                continue
            response_text = generate_recommendation_response(merged_query,df_recommend,user_name,is_retry=is_retry)
            print("ğŸ¤– GPT:\n", response_text)
            log_recommendations(interaction_id, df_recommend["title"].tolist())

            # ì¬ì¶”ì²œ ì´í›„ì—ë„ last_selected_title ì€
            # ì‚¬ìš©ìê°€ ì„ íƒí–ˆì„ ë•Œ ë³„ë„ ë¡œì§ìœ¼ë¡œ ì—…ë°ì´íŠ¸í•´ë‘ì„¸ìš”.
            last_recommend_df = df_recommend.copy()
            first_turn = False

        if first_turn and is_recommendation_request(query):
            print("first question")

            # â‘  user_meta í•œ ë²ˆë§Œ ì¶”ì¶œ
            user_meta = extract_user_meta(query)
            # (í•„ìš”í•˜ë‹¤ë©´ ì„¸ì…˜ì—ë„ ì €ì¥)
            last_user_meta = user_meta

            # â‘¡ ìˆ˜ì •ëœ handle_recommendation í˜¸ì¶œ
            #    ì‹œê·¸ë‹ˆì²˜: handle_recommendation(df, user_id, user_meta, selected_title=None)
            df_recommend = handle_recommendation(
                df,
                user_id,
                user_meta,
                selected_title  # ì²« ì¶”ì²œ ë‹¨ê³„ì´ë¯€ë¡œ ë³´í†µ None
            )

            if df_recommend.empty:
                print("ğŸ¤– GPT:\nì£„ì†¡í•´ìš”, ì ì ˆí•œ ì½˜í…ì¸ ë¥¼ ì°¾ì§€ ëª»í–ˆì–´ìš”.")
                continue

            response_text = generate_recommendation_response(query, df_recommend, user_name)
            print("ğŸ¤– GPT:\n", response_text)

            log_recommendations(interaction_id, df_recommend["title"].tolist())

            # â‘¢ ìƒíƒœ ì—…ë°ì´íŠ¸
            last_recommend_df    = df_recommend.copy()
            last_recommend_query = query
            first_turn           = False

            continue

        # âœ… ê¸°íƒ€ ì¼ë°˜ ì§ˆë¬¸ ì²˜ë¦¬
        else:
            print("ğŸŸ  ì¼ë°˜ ì§ˆë¬¸ìœ¼ë¡œ íŒë‹¨ â†’ QA ì²´ì¸ ì‹¤í–‰")
            try:
                result = qa_chain.invoke({"question": query})
                print("ğŸ¤– GPT:\n", result["answer"])
            except Exception as e:
                print("âŒ í† í° ì´ˆê³¼ ë˜ëŠ” ì²˜ë¦¬ ì˜¤ë¥˜ â†’ ì¼ë°˜ ì‘ë‹µ ë¶ˆê°€")
                print("ğŸ¤– GPT:\nì£„ì†¡í•´ìš”, í•´ë‹¹ ì§ˆë¬¸ì—ëŠ” ë‹µë³€í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

if __name__ == "__main__":
    main_chat_loop()
